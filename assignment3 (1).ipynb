{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats,special\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom xgboost import XGBRegressor\n\nimport lightgbm as lgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.cluster import KMeans, MeanShift, DBSCAN, Birch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nG = nx.Graph()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/citati/mappings_v10_ref_1.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = 8407630\n\nimport networkx as nx\nG = nx.Graph()\n\n#G.add_nodes_from(df[[1],[1]])\n\ny=df.iloc[1:size,0].values\n\nG.add_nodes_from(y)\n\n\n\nx=df.iloc[1:size,[0,1]].values\n\n\nG.add_edges_from(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G.number_of_edges()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G.number_of_nodes()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#G = nx.petersen_graph()\n#plt.subplot(121)\n##<matplotlib.axes._subplots.AxesSubplot object at ...>\n#nx.draw(G, with_labels=True, font_weight='bold')\n#plt.subplot(122)\n##<matplotlib.axes._subplots.AxesSubplot object at ...>\n##nx.draw_shell(G, nlist=[range(5, 10), range(5)], with_labels=True, font_weight='bold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine characteristics\n#nx.radius(G)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nx.density(G)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport math\n\ndegree_sequence = sorted([d for n, d in G.degree()], reverse=True)  # degree sequence\ndegreeCount = collections.Counter(degree_sequence)\ndeg, cnt = zip(*degreeCount.items())\n\nfig, ax = plt.subplots()\n#plt.bar(deg, cnt, width=0.80, color=\"b\")\nplt.bar(deg, cnt, width=2, color=\"b\")\n\nplt.title(\"Degree Histogram\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Degree\")\n#ax.set_xticks([d + 0.4 for d in deg])\nax.set_xticks([d for d in deg])\nax.set_xticklabels(deg)\n\nax.set_xlim([0,50])\n\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nx.degree_centrality(G)\n#cent = sorted(nx.degree_centrality(G))\n# #nx.voterank(G, 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this (remove comment) to find degree centrality ####################\n\n#cent = nx.degree_centrality(G)\n#s1 = sorted(cent.values(), reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sorted(cent.items(), key=lambda x:x[1], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ignore this\n#sorted(cent, key=cent.get)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Centrality Analysis","metadata":{}},{"cell_type":"code","source":"# change this from markdown to code to evaluate centrality\n\ncent2 = nx.approximate_current_flow_betweenness_centrality(G)\ns2 = sorted(cent2.values(), reverse=True)\nsorted(cent2.items(), key=lambda x:x[1], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Link Prediction\n","metadata":{}},{"cell_type":"code","source":"# draw network (not expecting this to work)\n#nx.draw_networkx(G, with_labels=False, node_size=50, node_color='r')\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.sparse as sp\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add definitions\n\nimport numpy as np\nimport scipy.sparse as sp\nimport networkx as nx\n\n# Convert sparse matrix to tuple\ndef sparse_to_tuple(sparse_mx):\n    if not sp.isspmatrix_coo(sparse_mx):\n        sparse_mx = sparse_mx.tocoo()\n    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n    values = sparse_mx.data\n    shape = sparse_mx.shape\n    return coords, values, shape\n\n# Get normalized adjacency matrix: A_norm\ndef preprocess_graph(adj):\n    adj = sp.coo_matrix(adj)\n    adj_ = adj + sp.eye(adj.shape[0])\n    rowsum = np.array(adj_.sum(1))\n    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n    return sparse_to_tuple(adj_normalized)\n\n# Prepare feed-dict for Tensorflow session\ndef construct_feed_dict(adj_normalized, adj, features, placeholders):\n    # construct feed dictionary\n    feed_dict = dict()\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['adj']: adj_normalized})\n    feed_dict.update({placeholders['adj_orig']: adj})\n    return feed_dict\n\n# Perform train-test split\n    # Takes in adjacency matrix in sparse format\n    # Returns: adj_train, train_edges, val_edges, val_edges_false, \n        # test_edges, test_edges_false\ndef mask_test_edges(adj, test_frac=.1, val_frac=.05, prevent_disconnect=True, verbose=False):\n    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n\n    if verbose == True:\n        print ('preprocessing...')\n\n    # Remove diagonal elements\n    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n    adj.eliminate_zeros()\n    # Check that diag is zero:\n    assert np.diag(adj.todense()).sum() == 0\n\n    g = nx.from_scipy_sparse_matrix(adj)\n    orig_num_cc = nx.number_connected_components(g)\n\n    adj_triu = sp.triu(adj) # upper triangular portion of adj matrix\n    adj_tuple = sparse_to_tuple(adj_triu) # (coords, values, shape), edges only 1 way\n    edges = adj_tuple[0] # all edges, listed only once (not 2 ways)\n    # edges_all = sparse_to_tuple(adj)[0] # ALL edges (includes both ways)\n    num_test = int(np.floor(edges.shape[0] * test_frac)) # controls how large the test set should be\n    num_val = int(np.floor(edges.shape[0] * val_frac)) # controls how alrge the validation set should be\n\n    # Store edges in list of ordered tuples (node1, node2) where node1 < node2\n    edge_tuples = [(min(edge[0], edge[1]), max(edge[0], edge[1])) for edge in edges]\n    all_edge_tuples = set(edge_tuples)\n    train_edges = set(edge_tuples) # initialize train_edges to have all edges\n    test_edges = set()\n    val_edges = set()\n\n    if verbose == True:\n        print ('generating test/val sets...')\n\n    # Iterate over shuffled edges, add to train/val sets\n    np.random.shuffle(edge_tuples)\n    for edge in edge_tuples:\n        # print edge\n        node1 = edge[0]\n        node2 = edge[1]\n\n        # If removing edge would disconnect a connected component, backtrack and move on\n        g.remove_edge(node1, node2)\n        if prevent_disconnect == True:\n            if nx.number_connected_components(g) > orig_num_cc:\n                g.add_edge(node1, node2)\n                continue\n\n        # Fill test_edges first\n        if len(test_edges) < num_test:\n            test_edges.add(edge)\n            train_edges.remove(edge)\n\n        # Then, fill val_edges\n        elif len(val_edges) < num_val:\n            val_edges.add(edge)\n            train_edges.remove(edge)\n\n        # Both edge lists full --> break loop\n        elif len(test_edges) == num_test and len(val_edges) == num_val:\n            break\n\n    if (len(val_edges) < num_val or len(test_edges) < num_test):\n        print (\"WARNING: not enough removable edges to perform full train-test split!\")\n        print (\"Num. (test, val) edges requested: (\", num_test, \", \", num_val, \")\")\n        print (\"Num. (test, val) edges returned: (\", len(test_edges), \", \", len(val_edges), \")\")\n\n    if prevent_disconnect == True:\n        assert nx.number_connected_components(g) == orig_num_cc\n\n    if verbose == True:\n        print ('creating false test edges...')\n\n    test_edges_false = set()\n    while len(test_edges_false) < num_test:\n        idx_i = np.random.randint(0, adj.shape[0])\n        idx_j = np.random.randint(0, adj.shape[0])\n        if idx_i == idx_j:\n            continue\n\n        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n\n        # Make sure false_edge not an actual edge, and not a repeat\n        if false_edge in all_edge_tuples:\n            continue\n        if false_edge in test_edges_false:\n            continue\n\n        test_edges_false.add(false_edge)\n\n    if verbose == True:\n        print ('creating false val edges...')\n\n    val_edges_false = set()\n    while len(val_edges_false) < num_val:\n        idx_i = np.random.randint(0, adj.shape[0])\n        idx_j = np.random.randint(0, adj.shape[0])\n        if idx_i == idx_j:\n            continue\n\n        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n\n        # Make sure false_edge in not an actual edge, not in test_edges_false, not a repeat\n        if false_edge in all_edge_tuples or \\\n            false_edge in test_edges_false or \\\n            false_edge in val_edges_false:\n            continue\n            \n        val_edges_false.add(false_edge)\n\n    if verbose == True:\n        print ('creating false train edges...')\n\n    train_edges_false = set()\n    while len(train_edges_false) < len(train_edges):\n        idx_i = np.random.randint(0, adj.shape[0])\n        idx_j = np.random.randint(0, adj.shape[0])\n        if idx_i == idx_j:\n            continue\n\n        false_edge = (min(idx_i, idx_j), max(idx_i, idx_j))\n\n        # Make sure false_edge in not an actual edge, not in test_edges_false, \n            # not in val_edges_false, not a repeat\n        if false_edge in all_edge_tuples or \\\n            false_edge in test_edges_false or \\\n            false_edge in val_edges_false or \\\n            false_edge in train_edges_false:\n            continue\n\n        train_edges_false.add(false_edge)\n\n    if verbose == True:\n        print ('final checks for disjointness...')\n\n    # assert: false_edges are actually false (not in all_edge_tuples)\n    assert test_edges_false.isdisjoint(all_edge_tuples)\n    assert val_edges_false.isdisjoint(all_edge_tuples)\n    assert train_edges_false.isdisjoint(all_edge_tuples)\n\n    # assert: test, val, train false edges disjoint\n    assert test_edges_false.isdisjoint(val_edges_false)\n    assert test_edges_false.isdisjoint(train_edges_false)\n    assert val_edges_false.isdisjoint(train_edges_false)\n\n    # assert: test, val, train positive edges disjoint\n    assert val_edges.isdisjoint(train_edges)\n    assert test_edges.isdisjoint(train_edges)\n    assert val_edges.isdisjoint(test_edges)\n\n    if verbose == True:\n        print ('creating adj_train...')\n\n    # Re-build adj matrix using remaining graph\n    adj_train = nx.adjacency_matrix(g)\n\n    # Convert edge-lists to numpy arrays\n    train_edges = np.array([list(edge_tuple) for edge_tuple in train_edges])\n    train_edges_false = np.array([list(edge_tuple) for edge_tuple in train_edges_false])\n    val_edges = np.array([list(edge_tuple) for edge_tuple in val_edges])\n    val_edges_false = np.array([list(edge_tuple) for edge_tuple in val_edges_false])\n    test_edges = np.array([list(edge_tuple) for edge_tuple in test_edges])\n    test_edges_false = np.array([list(edge_tuple) for edge_tuple in test_edges_false])\n\n    if verbose == True:\n        print ('Done with train-test split!')\n        print ('')\n\n    # NOTE: these edge lists only contain single direction of edge!\n    return adj_train, train_edges, train_edges_false, \\\n        val_edges, val_edges_false, test_edges, test_edges_false\n\n# Perform train-test split\n    # Takes in adjacency matrix in sparse format (from a directed graph)\n    # Returns: adj_train, train_edges, val_edges, val_edges_false, \n        # test_edges, test_edges_false\ndef mask_test_edges_directed(adj, test_frac=.1, val_frac=.05, \n    prevent_disconnect=True, verbose=False, false_edge_sampling='iterative'):\n    if verbose == True:\n        print ('preprocessing...')\n\n    # Remove diagonal elements\n    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n    adj.eliminate_zeros()\n    # Check that diag is zero:\n    assert np.diag(adj.todense()).sum() == 0\n\n    # Convert to networkx graph to calc num. weakly connected components\n    g = nx.from_scipy_sparse_matrix(adj, create_using=nx.DiGraph())\n    orig_num_wcc = nx.number_weakly_connected_components(g)\n\n    adj_tuple = sparse_to_tuple(adj) # (coords, values, shape)\n    edges = adj_tuple[0] # List of ALL edges (either direction)\n    edge_pairs = [(edge[0], edge[1]) for edge in edges] # store edges as list of tuples (from_node, to_node)\n\n    num_test = int(np.floor(edges.shape[0] * test_frac)) # controls how large the test set should be\n    num_val = int(np.floor(edges.shape[0] * val_frac)) # controls how alrge the validation set should be\n    num_train = len(edge_pairs) - num_test - num_val # num train edges\n\n    all_edge_set = set(edge_pairs)\n    train_edges = set(edge_pairs) # init train_edges to have all edges\n    test_edges = set() # init test_edges as empty set\n    val_edges = set() # init val edges as empty set\n\n    ### ---------- TRUE EDGES ---------- ###\n    # Shuffle and iterate over all edges\n    np.random.shuffle(edge_pairs)\n\n    # get initial bridge edges\n    bridge_edges = set(nx.bridges(nx.to_undirected(g))) \n\n    if verbose:\n        print('creating true edges...')\n\n    for ind, edge in enumerate(edge_pairs):\n        node1, node2 = edge[0], edge[1]\n\n        # Recalculate bridges every ____ iterations to relatively recent\n        if ind % 10000 == 0:\n            bridge_edges = set(nx.bridges(nx.to_undirected(g))) \n\n        # Don't sample bridge edges to increase likelihood of staying connected\n        if (node1, node2) in bridge_edges or (node2, node1) in bridge_edges: \n            continue\n\n        # If removing edge would disconnect the graph, backtrack and move on\n        g.remove_edge(node1, node2)\n        if prevent_disconnect == True:\n            if not nx.is_weakly_connected(g):\n                g.add_edge(node1, node2)\n                continue\n\n        # Fill test_edges first\n        if len(test_edges) < num_test:\n            test_edges.add(edge)\n            train_edges.remove(edge)\n            if len(test_edges) % 10000 == 0 and verbose == True:\n                print ('Current num test edges: ', len(test_edges))\n\n        # Then, fill val_edges\n        elif len(val_edges) < num_val:\n            val_edges.add(edge)\n            train_edges.remove(edge)\n            if len(val_edges) % 10000 == 0 and verbose == True:\n                print ('Current num val edges: ', len(val_edges))\n\n        # Both edge lists full --> break loop\n        elif len(test_edges) == num_test and len(val_edges) == num_val:\n            break\n\n\n\n    # Check that enough test/val edges were found\n    if (len(val_edges) < num_val or len(test_edges) < num_test):\n        print (\"WARNING: not enough removable edges to perform full train-test split!\")\n        print (\"Num. (test, val) edges requested: (\", num_test, \", \", num_val, \")\")\n        print (\"Num. (test, val) edges returned: (\", len(test_edges), \", \", len(val_edges), \")\")\n\n    # Print stats for largest remaining WCC\n    print ('Num WCC: ', nx.number_weakly_connected_components(g))\n    largest_wcc_set = max(nx.weakly_connected_components(g), key=len)\n    largest_wcc = g.subgraph(largest_wcc_set)\n    print ('Largest WCC num nodes: ', largest_wcc.number_of_nodes())\n    print ('Largest WCC num edges: ', largest_wcc.number_of_edges())\n\n    if prevent_disconnect == True:\n        assert nx.number_weakly_connected_components(g) == orig_num_cc\n\n    # Fraction of edges with both endpoints in largest WCC\n    def frac_edges_in_wcc(edge_set):\n        num_wcc_contained_edges = 0.0\n        num_total_edges = 0.0\n        for edge in edge_set:\n            num_total_edges += 1\n            if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set:\n                num_wcc_contained_edges += 1\n        frac_in_wcc = num_wcc_contained_edges / num_total_edges\n        return frac_in_wcc\n\n    # Check what percentage of edges have both endpoints in largest WCC\n    print ('Fraction of train edges with both endpoints in L-WCC: ', frac_edges_in_wcc(train_edges))\n    print ('Fraction of test edges with both endpoints in L-WCC: ', frac_edges_in_wcc(test_edges))\n    print ('Fraction of val edges with both endpoints in L-WCC: ', frac_edges_in_wcc(val_edges))\n\n    # Ignore edges with endpoint not in largest WCC\n    print ('Removing edges with either endpoint not in L-WCC from train-test split...')\n    train_edges = {edge for edge in train_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n    test_edges = {edge for edge in test_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n    val_edges = {edge for edge in val_edges if edge[0] in largest_wcc_set and edge[1] in largest_wcc_set}\n\n\n    ### ---------- FALSE EDGES ---------- ###\n\n    # Initialize empty sets\n    train_edges_false = set()\n    test_edges_false = set()\n    val_edges_false = set()\n\n    # Generate candidate false edges (from g-complement) and iterate through them\n    if false_edge_sampling == 'iterative':\n        if verbose == True:\n            print ('preparing complement adjacency matrix...')\n\n        # Sample false edges from G-complement, instead of randomly generating edges\n        # g_complement = nx.complement(g)\n        adj_complement = 1 - adj.toarray() # flip 0's, 1's in adjacency matrix\n        np.fill_diagonal(adj_complement, val=0) # set diagonals to 0\n\n        # 2 numpy arrays indicating x, y coords in adj_complement\n            # WARNING: This line can use up a lot of RAM depending on 'adj' size\n        idx1, idx2 = np.where(adj_complement == 1) \n            \n        edges_false = np.stack((idx1, idx2), axis=-1) # stack arrays into coord pairs.\n        edge_pairs_false = [(edge[0], edge[1]) for false_edge in edges_false]\n\n        # Shuffle and iterate over false edges\n        np.random.shuffle(edge_pairs_false)\n        if verbose == True:\n            print ('adding candidate false edges to false edge sets...')\n        for false_edge in edge_pairs_false:\n            # Fill train_edges_false first\n            if len(train_edges_false) < len(train_edges):\n                train_edges_false.add(false_edge)\n                if len(train_edges_false) % 100000 == 0 and verbose == True:\n                    print ('Current num false train edges: ', len(train_edges_false))\n\n            # Fill test_edges_false next\n            elif len(test_edges_false) < len(test_edges):\n                test_edges_false.add(false_edge)\n                if len(test_edges_false) % 100000 == 0 and verbose == True:\n                    print ('Current num false test edges: ', len(test_edges_false))\n\n            # Fill val_edges_false last\n            elif len(val_edges_false) < len(val_edges):\n                val_edges_false.add(false_edge)\n                if len(val_edges_false) % 100000 == 0 and verbose == True:\n                    print ('Current num false val edges: ', len(val_edges_false))\n\n            # All sets filled --> break\n            elif len(train_edges_false) == len(train_edges) and \\\n                len(test_edges_false) == len(test_edges) and \\\n                len(val_edges_false) == len(val_edges):\n                break\n\n    # Randomly generate false edges (idx_i, idx_j) 1 at a time to save memory\n    elif false_edge_sampling == 'random':\n        if verbose == True:\n            print ('creating false test edges...')\n\n        # FALSE TEST EDGES\n        while len(test_edges_false) < len(test_edges):\n            idx_i = np.random.randint(0, adj.shape[0])\n            idx_j = np.random.randint(0, adj.shape[0])\n            if idx_i == idx_j: # no self-loops\n                continue\n\n            # Ensure both endpoints are in largest WCC\n            if idx_i not in largest_wcc_set or idx_j not in largest_wcc_set:\n                continue\n\n            false_edge = (idx_i, idx_j)\n\n            # Make sure false_edge not an actual edge, and not a repeat\n            if false_edge in all_edge_set:\n                continue\n            if false_edge in test_edges_false:\n                continue\n\n            test_edges_false.add(false_edge)\n\n            if len(test_edges_false) % 100000 == 0 and verbose == True:\n                print ('Current num false test edges: ', len(test_edges_false))\n\n        # FALSE VAL EDGES\n        if verbose == True:\n            print ('creating false val edges...')\n\n        while len(val_edges_false) < len(val_edges):\n            idx_i = np.random.randint(0, adj.shape[0])\n            idx_j = np.random.randint(0, adj.shape[0])\n            if idx_i == idx_j:\n                continue\n\n            false_edge = (idx_i, idx_j)\n\n            # Make sure false_edge in not an actual edge, not in test_edges_false, not a repeat\n            if false_edge in all_edge_set or \\\n                false_edge in test_edges_false or \\\n                false_edge in val_edges_false:\n                continue\n                \n            val_edges_false.add(false_edge)\n\n            if len(val_edges_false) % 100000 == 0 and verbose == True:\n                print ('Current num false val edges: ', len(val_edges_false))\n\n        # FALSE TRAIN EDGES\n        if verbose == True:\n            print ('creating false train edges...')\n\n        while len(train_edges_false) < len(train_edges):\n            idx_i = np.random.randint(0, adj.shape[0])\n            idx_j = np.random.randint(0, adj.shape[0])\n            if idx_i == idx_j:\n                continue\n\n            false_edge = (idx_i, idx_j)\n\n            # Make sure false_edge in not an actual edge, not in test_edges_false, \n                # not in val_edges_false, not a repeat\n            if false_edge in all_edge_set or \\\n                false_edge in test_edges_false or \\\n                false_edge in val_edges_false or \\\n                false_edge in train_edges_false:\n                continue\n\n            train_edges_false.add(false_edge)\n\n            if len(train_edges_false) % 100000 == 0 and verbose == True:\n                print ('Current num false train edges: ', len(train_edges_false))\n\n\n    ### ---------- FINAL DISJOINTNESS CHECKS ---------- ###\n    if verbose == True:\n        print ('final checks for disjointness...')\n\n    # assert: false_edges are actually false (not in all_edge_tuples)\n    assert test_edges_false.isdisjoint(all_edge_set)\n    assert val_edges_false.isdisjoint(all_edge_set)\n    assert train_edges_false.isdisjoint(all_edge_set)\n\n    # assert: test, val, train false edges disjoint\n    assert test_edges_false.isdisjoint(val_edges_false)\n    assert test_edges_false.isdisjoint(train_edges_false)\n    assert val_edges_false.isdisjoint(train_edges_false)\n\n    # assert: test, val, train positive edges disjoint\n    assert val_edges.isdisjoint(train_edges)\n    assert test_edges.isdisjoint(train_edges)\n    assert val_edges.isdisjoint(test_edges)\n\n    if verbose == True:\n        print ('creating adj_train...')\n\n    # Re-build adj matrix using remaining graph\n    adj_train = nx.adjacency_matrix(g)\n\n    # Convert edge-lists to numpy arrays\n    train_edges = np.array([list(edge_tuple) for edge_tuple in train_edges])\n    train_edges_false = np.array([list(edge_tuple) for edge_tuple in train_edges_false])\n    val_edges = np.array([list(edge_tuple) for edge_tuple in val_edges])\n    val_edges_false = np.array([list(edge_tuple) for edge_tuple in val_edges_false])\n    test_edges = np.array([list(edge_tuple) for edge_tuple in test_edges])\n    test_edges_false = np.array([list(edge_tuple) for edge_tuple in test_edges_false])\n\n    if verbose == True:\n        print ('Done with train-test split!')\n        print ('Num train edges (true, false): (', train_edges.shape[0], ', ', train_edges_false.shape[0], ')')\n        print ('Num test edges (true, false): (', test_edges.shape[0], ', ', test_edges_false.shape[0], ')')\n        print ('Num val edges (true, false): (', val_edges.shape[0], ', ', val_edges_false.shape[0], ')')\n        print ('')\n\n    # Return final edge lists (edges can go either direction!)\n    return adj_train, train_edges, train_edges_false, \\\n        val_edges, val_edges_false, test_edges, test_edges_false","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import gae\n#from gae.preprocessing import mask_test_edges\nnp.random.seed(0) # make sure train-test split is consistent between notebooks\nadj_sparse = nx.to_scipy_sparse_matrix(G)\n\n# Perform train-test split\nadj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.1)\nG_train = nx.from_scipy_sparse_matrix(adj_train) # new graph object with only non-hidden edges","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspect train/test split\nprint \"Total nodes:\", adj_sparse.shape[0]\nprint \"Total edges:\", int(adj_sparse.nnz/2) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\nprint \"Training edges (positive):\", len(train_edges)\nprint \"Training edges (negative):\", len(train_edges_false)\nprint \"Validation edges (positive):\", len(val_edges)\nprint \"Validation edges (negative):\", len(val_edges_false)\nprint \"Test edges (positive):\", len(test_edges)\nprint \"Test edges (negative):\", len(test_edges_false)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}